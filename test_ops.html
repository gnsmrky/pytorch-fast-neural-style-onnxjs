<html>
    
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>PyTorch fast-neural-style running in web browser</title>

<script>
        const cOnnxjsVersion="v0.1.5"; // set this to the same version string as in npm url for onnx.min.js
</script>
<script src="https://cdn.jsdelivr.net/npm/onnxjs@0.1.5/dist/onnx.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/ua-parser-js@0/dist/ua-parser.min.js"></script>

<body onload="htmlGenerateUI()">
<font face="verdana">
<h1>ONNX.js op test</h1>

<table>
    <tr>
        <td align='right'>Backend:<br/>&nbsp;</td>
        <td>
            <select id="backendSelectElem" onchange="htmlOnBackendSelectChange()">
                <option value='webgl'  selected='selected'>WebGL</option>
                <option value='cpu'>CPU</option>
                <option value='cpu'>Web Assembly</option>
            </select>
            <br/>&nbsp;
        </td>
    </tr>
    <tr>
        <td align='right'>ONNX Op model:</td>
        <td>
            <select id="opModelSelectElem" onchange="htmlOnOpModelSelectChange()"></select>
        </td>
    </tr>
    <tr>
        <td align='right' valign='top'>ONNX Graph:</td>
        <td><img id='onnxGraphElem' width='100px' src=''/></td>
    </tr>
</table>

<br/>

<input type="button" id="testButtonElem" value="Run Op Test" onclick="htmlOnRunOpTest()"/>

<p>
<div id='logOutputDiv'></div>
</p>

<div id="copyButtonDiv"></div>

</font>

<script>
const cBackendSelectId = "backendSelectElem";
const cOpModelSelectId = "opModelSelectElem";

const cModelList = [
    { name: "Conv2d"                        , model_url: "./onnx_test/test_conv2d_128x128.onnx" },
    { name: "Pad 'Reflection' mode"         , model_url: "./onnx_test/test_pad_reflect_128x128.onnx" },
    { name: "Pad 'Reflection' mode + Conv2D", model_url: "./onnx_test/test_pad_reflect_conv2d_128x128.onnx" },
    { name: "Pad 'Zero' mode"               , model_url: "./onnx_test/test_pad_zero_128x128.onnx" },
];


const cNewLine = String.fromCharCode(13, 10);

function asyncSetHtml (elemNode, html) {
  var p = new Promise( (resolve, reject) => {
    elemNode.innerHTML = html;
    setTimeout (resolve, 0);
  });

  return p;
}

function htmlGenerateUI () {
    htmlGenerateModelList(opModelSelectElem);
}

function htmlGenerateModelList (listElem) {
    listElem.innerHTML = "";
    const list = cModelList;
    for (i=0; i<list.length; i++) {
        if (i==0) {
            listElem.innerHTML += "<option value='" + i + "' selected='selected'>" + list[i].name + "</option>";
        } else {
            listElem.innerHTML += "<option value='" + i + "'>" + list[i].name + "</option>";
        }
    }

    onnxGraphElem.src = list[0].model_url + ".png";
}

function htmlOnBackendSelectChange() {
    
}

function htmlOnOpModelSelectChange() {
    const modelIdx = opModelSelectElem.value;
    
    const list = cModelList;
    onnxGraphElem.src = list[modelIdx].model_url + ".png";
}

function htmlOnCopyToClipboard (textAreaElem) {
    //outputElem = document.getElementById(textAreaId);
    textAreaElem.select();
    document.execCommand("copy");
}

function htmlOnRunOpTest() {
    // log browser info
    var uap = new UAParser();
    uap.setUA(navigator.userAgent);
    var uapRes = uap.getResult();
    
    inferResultStr  = "ONNX.js Op Test" + cNewLine;
    inferResultStr += "======================================================================" + cNewLine;
    inferResultStr += "ONNX.js " + cOnnxjsVersion + cNewLine;
    inferResultStr += cNewLine;
    inferResultStr += "os: "      + uapRes.os.name      + " " + uapRes.os.version      + cNewLine;
    inferResultStr += "browser: " + uapRes.browser.name + " " + uapRes.browser.version + cNewLine;
    inferResultStr += "engine: "  + uapRes.engine.name  + " " + uapRes.engine.version  + cNewLine;

    inferResultStr += cNewLine;

    // log cpu arch info
    inferResultStr += "cpu arch: " + uapRes.cpu.architecture + cNewLine;

    // log gpu info
    var glCtx = glcanvas.getContext("webgl") || glcanvas.getContext("experimental-webgl");
    if (glCtx == null) {
        inferResultStr += "cannot get 'webgl' context..." + cNewLine;
    } else {
        var glInfo = glCtx.getExtension("WEBGL_debug_renderer_info");
        if (glInfo != null) {
        inferResultStr += "gpu: " + glCtx.getParameter(glInfo.UNMASKED_RENDERER_WEBGL) + cNewLine;
        } else {
        inferResultStr += "gpu: unknown" + cNewLine;
        }
    }
    inferResultStr += cNewLine;

    // log backend info
    backend = backendSelectElem.value;
    inferResultStr += "ONNX.js backend: " + backend + cNewLine;
    
    // log onnx model file
    const opModelIdx = document.getElementById(cOpModelSelectId).value;
    const modelUrl = cModelList[opModelIdx].model_url;
    inferResultStr += "ONNX model file: " + modelUrl + cNewLine + cNewLine;

    // create text area
    const logOutputElemId = "resultLogElem";
    logOutputDiv.innerHTML = "<textarea id='" + logOutputElemId + "' readonly cols=90 rows=30></textarea>";

    // add copy button
    copyButtonDiv.innerHTML = "<button onclick='htmlOnCopyToClipboard(" + logOutputElemId + ")'>Copy to clipboard</button>";

    // load & run inference
    logOutputElem = document.getElementById(logOutputElemId);
    asyncSetHtml (logOutputElem, inferResultStr).then (runOpTest(logOutputElem));
}

function runOpTest (outputElem) {
    
    const backend = document.getElementById(cBackendSelectId).value;
    const opModelIdx = document.getElementById(cOpModelSelectId).value;
    
    const sess = new onnx.InferenceSession({backendHint: backend});

    const modelUrl = cModelList[opModelIdx].model_url;

    sess.loadModel(modelUrl).then(()=>{
        const n = 3;
        const h = 128;
        const w = 128;

        const x = new Float32Array(1*n*h*w);

        for (i=0; i<1*n*h*w; i++) {
            x[i] = (i%256) / 255.0;
        }

        const inputT = new onnx.Tensor(x, 'float32', [1,n,h,w]);
        
        outputElem.innerHTML += "output:" + cNewLine;
        outputElem.innerHTML += "----------------------------------------------------------------------" + cNewLine;
        sess.run([inputT]).then(output=>{
            const outputT = output.values().next().value;

            outputElem.innerHTML += "model output tensor size:" + outputT.size + cNewLine + cNewLine;
            
            // display first few floats
            var i;
            const cNumValues = 16
            for (var i=0; i<cNumValues; i++) {
                outputElem.innerHTML += "output.data[" + i + "]: " + outputT.data[i] + cNewLine;
            }
            outputElem.innerHTML += "... ... ..." + cNewLine;
            //outputElem.innerHTML += cNewLine;

            // display few floats in the middle of the image
            const cSkip = h*w/2;
            for (var j=0; j<cNumValues; j++) {
                const i = j + cSkip;
                outputElem.innerHTML += "outout.data[" + i + "]: " + outputT.data[i] + cNewLine;
            }
            outputElem.innerHTML += "... ... ..." + cNewLine;
            outputElem.innerHTML += cNewLine;
        }).catch ((e) => {
            outputElem.innerHTML += "!!!inference error!!! " + cNewLine + e + cNewLine;
        });
    }).catch((e)=> {
        outputElem.innerHTML += "!!!load error!!! " + cNewLine + e + cNewLine;
    });

}
</script>

  <!-- zero width & height canvas to get gpu info -->
  <canvas id="glcanvas" width="0" height="0"></canvas>
</body>
</html>

